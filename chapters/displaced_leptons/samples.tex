\section{Data and simulated samples}
\label{samples}
\fxnote{add intro blurb?}
\subsection{Experimental data}
This analysis uses proton-proton collision data taken in 2016, 2017, and 2018 at a center-of-mass energy of \SI{13}{\TeV}. In 2016, we use only the last two run periods due to lower displaced tracking efficiency caused by an analog pipeline voltage saturation problem in the silicon strip detector during the earlier run periods (see appendix~\ref{k0}). In 2017, we use all run periods in the $\Pe\Pe$ channel and all but the earliest run period in the $\Pe\Pgm$ and $\Pgm\Pgm$ channels because the $\Pe\Pgm$ and $\Pgm\Pgm$ triggers are not available in the earliest run period. In 2018, we use all available run periods in all three channels. Ultimately, this analysis uses an integrated luminosity of \SI{16.1\pm0.4}{\fb} from 2016 in all three channels, \SI{41.5\pm1.0}{\fb} (\SI{36.7\pm0.8}{\fb}) from 2017 in the $\Pe\Pe$ channel ($\Pe\Pgm$ and $\Pgm\Pgm$ channels), and \SI{59.7\pm1.5}{\fb} from 2018 in all three channels.

The search is performed in the \texttt{MuonEG}, \texttt{DoubleEG} (in 2016--2017), \texttt{EGamma} (in 2018), and \texttt{DoubleMu} primary datasets. We also use the \texttt{MET} dataset to study the trigger efficiency and the \texttt{Cosmics} and \texttt{NoBPTX} datasets to study the displaced tracking efficiency. All data are reconstructed in the \texttt{07Aug17}, \texttt{31Mar2018}, \texttt{17Sep2018} reprocessing campaigns with software versions \texttt{CMSSW\_8\_0\_31}, \texttt{CMSSW\_9\_4\_8}, and \texttt{CMSSW\_10\_2\_0}, respectively. The sole exception is the \texttt{EGamma} 2018D dataset, which was reconstructed in the \texttt{22Jan2019} campaign. In all cases, we use the CMS \texttt{MiniAOD} event format.

\subsection{Simulated background events}
\label{bg_samples}
This analysis employs a fully data-driven background estimation technique that does not rely on simulated SM events. We do, however, use simulated SM events to study possible sources of background and verify the validity of the background estimation technique. The samples corresponding to 2016, 2017, and 2018 data conditions are from the \texttt{PdmVMCcampaignRunIISummer16}, \texttt{PdmVMCcampaignRunIIFall17}, and \texttt{PdmVMCcampaignRunIIAutumn18} production campaigns and were reconstructed in \texttt{CMSSW\_8\_0\_31}, \texttt{CMSSW\_9\_4\_8}, \texttt{CMSSW\_10\_2\_0} with the MiniAODSIM event format. The samples simulating $Z$+jets, $W$+jets, and \ttbar production are generated using \MGvATNLO~\cite{madgraph,fxfx,mlm}, while the samples simulating diboson ($WW$, $WZ$, and $ZZ$ with leptonic and semi-leptonic decays) and single-top-quark production are simulated with \POWHEG v2~\cite{Frixione:2002ik,Nason:2004rx,Frixione:2007vw,Alioli:2008gx,Alioli:2010xd}. \PYTHIA 8.2~\cite{PYTHIA8} is used to simulate the parton showering and hadronization for all processes. The modeling of the underlying event is generated using the CUETP8M1~\cite{Khachatryan:2015pea} and CP5 tunes~\cite{Sirunyan:2019dfx} for simulated samples corresponding to the 2016 and 2017--18 data sets, respectively.

\subsection{Simulated signal events}

We use simulated signal events to guide the analysis strategy and interpret our results. Samples of simulated $\Pp\Pp {\to} \PSQt \PASQt$ events in which the top squarks decay to a lepton and a $\cPqb$ quark or $\cPqd$ quark, are produced at leading order using \PYTHIA 8.2~\cite{PYTHIA8}. For simplicity, all lepton flavors are generated with equal branching fractions. The top squarks can form strongly-produced hadronic states called R-hadrons, which are generated with \PYTHIA. The interactions of the R-hadrons with matter are not simulated in \GEANTfour, but they are expected to have a negligible impact on the analysis because the lepton identification requirements effectively require the R-hadron to decay in the middle of the tracker volume. Each R-hadron therefore traverses $\lesssim1$ interaction length, making it unlikely to produce a high quality track, come to a stop in the detector, or flip its charge. To generate the samples, we start with a SUSY Les Houches Accord file~\cite{LesHouches} corresponding to Snowmass Points and Slopes point 1a~\cite{snowmass_points_slopes} and modify the mass and width of the top squark according to the sample being produced. We generate samples with $\PSQt$ masses from \num{100} to \SI{1800}{\GeV} at \SI{100}{\GeV} intervals and with $\PSQt$ lifetimes at each decade from \SI{0.1}{\mm} to \SI{1}{m}. After producing these samples, we also employ a lifetime reweighting technique to effectively produce eight additional lifetime points between each pair of adjacent lifetimes. In the case of the 1~m samples, we also use an equivalent technique to effectively produce nine additional lifetime points between \num{1} and \SI{10}{\m}. The production cross sections for each $\PSQt$ mass hypothesis are taken from the website of the LHC SUSY Cross Section Working Group. The signal samples are reconstructed in the same campaigns and with the same conditions as the SM background samples described in \ref{bg_samples}.
\fxnote{cite something for the xs or add a table or both}

\fxnote{In addition to the signal samples described above, we also interpret our results with $\Pp\Pp \to \sLep\sLepbar \to \PAl\PXXSG\:\Pl\PXXSG$ and $\Pp\Pp \to \PH \to \mathrm{S}\mathrm{S} \to \Pl\PAl\:\Pl\PAl$ processes. The simulated $\Pp\Pp \to \sLep\sLepbar \to \PAl\PXXSG\:\Pl\PXXSG$ samples are generated at leading order using \MGvATNLO, and the simulated $\Pp\Pp \to \PH \to \mathrm{S}\mathrm{S} \to \Pl\PAl\:\Pl\PAl$  samples are generated using \POWHEG v2 and \PYTHIA 8.2 at next-to-leading order.}

\pagebreak